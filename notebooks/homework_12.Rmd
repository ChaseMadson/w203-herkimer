---
title: 'Part 2: CLM Practice (Homework 12)'
subtitle: 'Lab 2: What Makes a Product Successful? - W203 Section 8'
author: 'Team Herkimer: Rick Chen, Chase Madson, Maria Manna, Jash Sompalli'
date: '`r format(Sys.time(), "%b %d, %Y")`'
output:
  pdf_document:
    number_sections: true
    toc: true
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(stargazer)
library(lmtest)

knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.width = 4, fig.height = 2.5, fig.align = 'center') 


```

```{r load_data, include=FALSE}
videos <- 
  '../data/external/videos.txt' %>% 
  read_tsv(col_types = 'cciciidii')

```

   

We are given a data set on YouTube videos and want to run the model $ln(\text{views}) = \beta_0 + \beta_1 \text{rate} + \beta_3 \text{length}$, where the variables mean the following:

-   `views`: the number of views by YouTube users.
-   `rate`: This is the average of the ratings that the video received. You may think of this as a proxy for video quality.
-   `length`: the duration of the video in seconds.

The resulting model is summarized as follows:

```{r model_fit, results='asis', echo=FALSE}
fit <- lm(data = videos, formula = log(views) ~ rate + length)

stargazer(fit, 
          header = FALSE, 
          type = 'latex', 
          title = 'Regressing Log-Views on Rate and Length', 
          ci=TRUE, 
          ci.level=0.95, 
          single.row=TRUE, 
          omit.stat=c('adj.rsq', 'ser'))

```

\break

# Evaluate the IID Assumption

For this assumption to be met we expect the data to have been drawn from an independent and identically distributed sampling process. The unit of observation of this data set is a distinct YouTube video, and the creators of this sample of YouTube videos describe their sampling process as follows:

> *We consider all the YouTube videos to form a directed graph, where each video is a node in the graph. If a video b is in the related video list (first 20 only) of a video a, then there is a directed edge from a to b .... When processing each video, it checks the list of related videos and adds any new ones to the queue.*[^1]

[^1]: From *Dataset for "Statistics and Social Network of YouTube Videos"* <https://netsg.cs.sfu.ca/youtubedata/>

By this description, we know that each video was selected due to its relation to a previously selected video included in the sample. There is an explicit correlation between units of observation and a clear violation of the IID assumption, specifically *independence*.

Moreover, the creators state that they began pulling their sample from an "initial set of videos from the list of 'Recently Featured', 'Most Viewed', 'Top Rated', and 'Most Discussed'...", each one of these likely representing a cluster within the population of YouTube videos. This means the first subset of videos comes from one of these four clusters, and then subsequent data was pulled based on how closely related they were to this first subset. Therefore, we have major presence of clustering in our sample in contrast to the population, another violation of the IID assumption.

When considering whether the observations of this data were drawn from an identical distribution, we cannot easily conclude that the population distribution is the same over time. The method of selecting data heavily relies on the initial set of videos, which comes from the snapshot data of 'Recently Featured', 'Most Viewed', 'Top Rated', and 'Most Discussed' in Feb 22nd 2007. There might be a trend of what people like to see or discuss in a certain period of time and would change in a different time period. Thus, there is a possibility that more than one population distribution presents at different times of selecting data. The “Butterfly effect” of using the initial set of videos will even amplify the data selection biases. 

Finally, it is concerning to look at distribution of the `age` variable and see a complete absence of videos between 1 and 325 days old, which is likely a result of the "breadth-first" search algorithm. This tells us that the sampling approach has holes in it and does not resemble a simple random sample.

```{r iid_age, fig.cap = 'Unexplained Gap in Videos Aged between 1 and 325 Days Old', echo=FALSE}
videos %>%
  filter(!is.na(age)) %>% 
  ggplot(mapping = aes(x = age, y = log(views))) + 
  geom_point() +
  geom_smooth() + 
  labs(title = 'Scatter Plot of Log-Views and Video Age', 
       x = 'Age of Video in Days', 
       y = 'Number of Views (Log-Transformed)')

```

**Conclusion: No, the IID assumption is *not* met.**

\break

# Linear Conditional Expectation Assumption

```{r df_lce, include=FALSE}
df_lce <- 
  videos %>% 
  filter(!is.na(rate) & !is.na(length) & !is.na(views)) %>% 
  select(length, rate) %>% 
  mutate(residuals = fit$residuals, 
         predicted = fit$fitted.values)
```

For this assumption to be met we expect to observe no trend (i.e., a horizontal line) when observing the scatter plot between the model's fitted values and its residuals.

```{r lce_residual_fitted, fig.cap = 'Unusual Trends are Apparent Moving Left to Right', echo=FALSE}
df_lce %>% 
  ggplot(mapping = aes(x = predicted, y = residuals)) + 
  geom_point() +
  geom_smooth() + 
  labs(title = 'Scatterplot of Residuals vs. Predicted', 
       x = 'Values Predicted by the Model', 
       y = 'Model Residuals')

```

Here instead of a horizontal line we see an unusual trend in the residuals as the predictions increase. This model is systematically overestimating the values at the higher end, with a dramatic rise and dip occurring in the mid-section. This means there is a conditional relationship between residuals and fitted values depending on where we are in the range of predicted values. Therefore, we observe some violation of the linear conditional expectation assumption. 

To take a deeper look, let's plot the residuals individually against the two predictors `rate` and `length`.

```{r lce_residual_rate, fig.cap = 'Unusual Oscillations in Residuals as Rate Increases', echo=FALSE}
df_lce %>% 
  ggplot(mapping = aes(x = rate, y = residuals)) + 
  geom_point() +
  geom_smooth() + 
  labs(title = 'Scatterplot of Residuals vs. Rate', 
       x = 'Rate of the Video', 
       y = 'Model Residuals')

```

When plotting the residuals against the predictor `rate`, we see the source of the dramatic rise and dip in the mid-section of the previous plot.

```{r lce_residual_length, fig.cap = 'Model Not Accounting for Diminishing Returns in Video Length', echo=FALSE}
df_lce %>% 
  ggplot(mapping = aes(x = length, y = residuals)) + 
  geom_point() +
  geom_smooth() + 
  labs(title = 'Scatterplot of Residuals vs. Length', 
       x = 'Length of the Video', 
       y = 'Model Residuals')

```

When plotting residuals vs. the predictor `length`, we see the source of the slow downward curve seen in the residual vs. fitted plot. Reasonably speaking, the length of a video likely contributes diminishing returns towards the number of views it receives, for which a linear relationship does not extrapolate well.  

**Conclusion: No, the assumption is *not* met.**

\break

# No Perfect Colinearity Assumption

For this assumption to be met we expect to find no evidence of a perfect linear relationship between our two predictor variables, `rate` and `length`. To test for this, we plot these two predictors against each other.

```{r colinearity, fig.cap = 'No Evidence of Perfect Colinearity of Predictors', echo=FALSE}
videos %>% 
  ggplot(mapping = aes(x = length, y = rate)) + 
  geom_point() +
  geom_smooth() + 
  labs(title = 'Scatterplot of Predictor Variables', 
       x = 'Length of the Video', 
       y = 'Rate of the Video')

```

We observe no perfect colinearty between the two predictors. There is not even a hint of near-perfect colinearity. Thus, this assumption is met.

**Conclusion: Yes, the assumption is met.**

\break

# Homoskedastic Errors Assumption

For this assumption to be met we expect the variance of the residuals to be constant across the range of predicted values. To test for this, we will look at the scatter plot of model residuals against its fitted values and observe how the variance changes moving left to right. 

```{r non_constant_variance, fig.cap = 'Errors Do Not Appear to be Homoskedastic', echo=FALSE}
df_lce %>% 
  ggplot(mapping = aes(x = predicted, y = residuals)) + 
  geom_point() +
  labs(title = 'Scatterplot of Residuals vs. Predicted', 
       x = 'Values Predicted by the Model', 
       y = 'Model Residuals')

```

In the residual vs. predicted plot, we see what appears to be evidence of heteroskedastic errors. Moving from left to right along the range of predicted values, we clearly see the variance span [-4, 4], then shift to [-2, 4], then grow to [-5, 5], and finish with [-4, 2]. These fluctuations in variance across the range of predicted values is a clear indication of heteroskedasticity.

The Breusch-Pagan test is another way to evaluate for the presence of heteroskedasticity. This test evaluates the null hypothesis: "no evidence for heteroskedastic error variance", and so rejecting the null means we have evidence of a problem.

```{r non_constant_variance_bptest}
bptest(fit)

```

With a p-value well below 0.05, we reject the null hypothesis. The evidence suggests we cannot rule out the presence of heteroskedasticity.

**Conclusion: No, the assumption is *not* met.**

\break

# Normally Distributed Errors Assumption

```{r df_normal_errors, include=FALSE}
df_normal_errors <- 
  videos %>% 
  filter(!is.na(views) & !is.na(length) & !is.na(rate)) %>% 
  mutate(residuals = fit$residuals) 

```

For this assumption to be met we expect the residuals to be normally distributed. To test for this we look at a histogram of the errors plotted against a reference normal distribution. We also look at the Q-Q plot.

```{r normal_errors, fig.cap = 'Errors Appear to be Normally Distributed', echo=FALSE}
df_normal_errors %>% 
  ggplot(mapping = aes(x = residuals)) + 
  geom_histogram(aes(y = ..density..), bins = 50, fill = 'lightgrey', color = 'black') + 
  stat_function(fun = dnorm, geom = 'line', size = 0.75, color = 'cornflowerblue', 
                args = list(mean = mean(df_normal_errors$residuals), 
                            sd = sd(df_normal_errors$residuals))) + 
  labs(title = 'Distribution of Residuals vs. Theoretical Normal Distribution', 
       y = 'Density', 
       x = 'Residuals')

df_normal_errors %>%
  ggplot(mapping = aes(sample = residuals)) + 
  stat_qq() + 
  stat_qq_line() + 
  labs(title = 'Normal Q-Q', 
       y = 'Model Residuals', 
       x = 'Theoretical Quantiles')
  
```

Neither plots show a concerning amount of deviation from normal. Based on the histogram and Q-Q plot, we see a residual distribution that is pretty faithful to the normal distribution. 

**Conclusion: Yes, the assumption is met.**
